<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Projects</title>
</head>

<body>
    <center>
        <img src="https://media-exp1.licdn.com/dms/image/C5603AQFYG1ZcA-8uGA/profile-displayphoto-shrink_400_400/0?e=1606348800&v=beta&t=iXYmrR4uaQfZvPn8zIvmg5gPQOez6Zlq7cp8j2TTbRk"
            alt="Osman Badgail profile picture">
        <h1>Osman Badgail</h1>
    </center>
    <h2>Projects</h2>
    <p> <em>University of Western Ontario, London, Ontario | 09/2019 - 08/2020.</em> </p>
    <ol>
        <li> <strong> <a href="https://github.com/osman-95/Project_Progress_2/tree/master/Final_Model">Real-time Object
                    detection-based Image Classification</a> </strong> </li>
        <ul>
            <li>The objective of the project was to design a reliable real-time image classification model based on the
                output of the object detector. The model identifies the holistic scene based on the object detected in
                that scene. </li>
            <li>The design was divided into two stages;
                <ul>
                    <li>Object detection trained on an image dataset and </li>
                    <li>Image classification trained on the object detection output.</li>
                </ul>
            </li>

            <li>The object detection was built and trained on a pre-trained MobilnetV2 model; a high processing speed
                model reliable for real-time execution. The model was trained on an image dataset annotated with the
                objects labels to be trained on.</li>
            <li>The classification model was trained on the dataset built from the annotations that contain the
                information regarding the occurrence of an object along with its position coordinate and their
                respective class it belongs to. </li>
            <li>The complete model initially takes in the live image/frame as an input apply the trained object
                detection model to detect the objects and feed it to the second stage of the model (i.e. classification
                model) that process the object detection output using the trained classification model to detect or
                classify the holistic image. </li>
        </ul>
        <li> <strong>Supermarket Sales Forecasting Using Machine Learning in Python</strong> </li>
        <ul>
            <li>The objective of the project was to build a predictive machine-learning model to forecast the weekly
                sales of Walmart supermarkets.</li>
            <li>The model was built from the dataset of the previous sale data provided. The datasets consisted of
                information about store type, size and department it belongs to in addition to the other several factors
                related to the environment of the place (unemployment rate, fuel price, etc.).</li>
            <li>Regression models were built, trained, tuned and tested on several machine-learning algorithms mainly
                Decision Tree, Extra Tree, K-Nearest Neigbhor, Multilayer Feed-Forward Neural Network, etc.</li>

        </ul>

    </ol>
    <p> <em>Osmania University, Hyderabad, India | 2014-2018.</em> </p>
    <ol start="3">
        <li> <strong>Design and Development of a waste sorting system</strong> </li>
        <ul>
            <li>The project presents the design of a municipal solid waste separation facility that uses various sensors
                of different properties to segregate the solid wastes (Glass, Plastic, Metal).</li>
            <li>The design model contained various sensors (Capacitive sensor, Inductive sensor, IR sensor) to
                differentiate the object to be detected. </li>
            <li>Different objects to be sorted are placed on the conveyor belt and based on the sensor triggered the
                object is sorted accordingly.</li>
        </ul>
        <li> <strong>BB8 Droid (Wireless spherical Robot)</strong> </li>
        <ul>
            <li>The Robot constitutes a wireless spherical shell encapsulating the entire control and movement mechanism
                within the shell.</li>
            <li>The robotâ€™s movement was programmed on Arduino Uno (ATmega328P).</li>
            <li>The robot communicates with a phone remotely through Bluetooth Low Energy (LE) acting as a
                remote-control unit.</li>

        </ul>
    </ol>
</body>

</html>